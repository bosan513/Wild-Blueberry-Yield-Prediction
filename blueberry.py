# -*- coding: utf-8 -*-
"""blueberry.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ckunx1wjUvSBAgJ4O3YyqanIY_KfbI5S
"""

#16 Features Unit Description:
# Clonesize m2: The average blueberry clone size in the field
# Honeybee bees/m2/min: Honeybee density in the field
# Bumbles bees/m2/min: Bumblebee density in the field
# Andrena bees/m2/min: Andrena bee density in the field
# Osmia bees/m2/min: Osmia bee density in the field
# MaxOfUpperTRange ℃: The highest record of the upper band daily air temperature during the bloom season
# MinOfUpperTRange ℃: The lowest record of the upper band daily air temperature
# AverageOfUpperTRange ℃: The average of the upper band daily air temperature
# MaxOfLowerTRange ℃: The highest record of the lower band daily air temperature
# MinOfLowerTRange ℃: The lowest record of the lower band daily air temperature
# AverageOfLowerTRange ℃: The average of the lower band daily air temperature
# RainingDays Day: The total number of days during the bloom season, each of which has precipitation larger than zero
# AverageRainingDays Day: The average of raining days of the entire bloom season
# Fruitset: the percentage of blossoms which end up forming fruits
# Fruitmass: The average mass of fruit
# Seeds: The number of seeds in a set

# Prediction Target:
# Yield

import numpy as np
import pandas as pd
import seaborn as sns
from sklearn import preprocessing as pp
from sklearn import model_selection, tree, metrics
import matplotlib.pyplot as plt

origin = pd.read_csv("WildBlueberryPollinationSimulationData.csv")
origin.head()

origin.info()

dataset = origin.drop(['Row#'],axis=1)
dataset.head()

dataset.shape

dataset.isnull().sum()

print("Duplicated Data:",dataset.duplicated().sum())

target = dataset['yield']
target

# EDA

plt.figure(figsize=(5,5))
sns.boxplot(x='yield', data=dataset)
plt.show()

# matplotlib subplot for the categorical feature
nominal_df = dataset[['MaxOfUpperTRange','MinOfUpperTRange','AverageOfUpperTRange','MaxOfLowerTRange',
               'MinOfLowerTRange','AverageOfLowerTRange','RainingDays','AverageRainingDays']]

fig, ax = plt.subplots(2,4, figsize=(20,13))
for e, col in enumerate(nominal_df.columns):
    if e<=3:
        sns.boxplot(data=dataset, x=col, y='yield', ax=ax[0,e])
    else:
        sns.boxplot(data=dataset, x=col, y='yield', ax=ax[1,e-4])
plt.show()

plt.figure(figsize=(15,10))
plt.subplot(2,3,1)
plt.hist(dataset['bumbles'])
plt.title("Histogram of bumbles column")
plt.subplot(2,3,2)
plt.hist(dataset['andrena'])
plt.title("Histogram of andrena column")
plt.subplot(2,3,3)
plt.hist(dataset['osmia'])
plt.title("Histogram of osmia column")
plt.subplot(2,3,4)
plt.hist(dataset['clonesize'])
plt.title("Histogram of clonesize column")
plt.subplot(2,3,5)
plt.hist(dataset['honeybee'])
plt.title("Histogram of honeybee column")
plt.show()

corr = dataset.corr()
plt.figure(figsize=(15,10))
sns.heatmap(corr, vmin=-1, vmax=1, center=0, square=False, annot=True, cmap='coolwarm')
plt.show()

from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.spatial.distance import squareform
def distance(data):
    #thanks to @sergiosaharovsky for the fix
    corr = data.corr(method = 'spearman')
    dist_linkage = linkage(squareform(1 - abs(corr)), 'complete')

    plt.figure(figsize = (10, 8), dpi = 300)
    dendro = dendrogram(dist_linkage, labels=data.columns, leaf_rotation=90)
    plt.title(f'Feature Distance', weight = 'bold', size = 16)
    plt.show()

distance(dataset)

# remove highly correlated features (TRange) and replace them with a new feature (AverageTRange)
dataset = origin.drop(['Row#'],axis=1)
dataset['AverageTRange'] = (dataset['AverageOfUpperTRange'] +
                dataset['AverageOfLowerTRange'])/2

dataset2 = dataset.drop(['MaxOfUpperTRange','MinOfUpperTRange',
              'MaxOfLowerTRange','MinOfLowerTRange',
              'AverageOfUpperTRange','AverageOfLowerTRange',
              'AverageRainingDays'],1)
dataset2

corr = dataset2.corr()
plt.figure(figsize=(15,10))
sns.heatmap(corr, vmin=-1, vmax=1, center=0, square=False, annot=True, cmap='coolwarm')
plt.show()

dataset2.hist(bins=12,            # Total Number of bins
          edgecolor='black',  # line color
          linewidth=1.0,      # edge line width
          xlabelsize=8,       # x axis label size
          ylabelsize=8,       # y axis label size
          grid=False,         # no grid inbetween
          figsize = (10,8))   # Figure size

plt.tight_layout(rect=(0, 0, 1.2, 1.2))   # Makes our plot looks cleaner

# Transform
mms = pp.MinMaxScaler()
trans_dataset = pd.DataFrame(mms.fit_transform(dataset2),columns=dataset2.columns)
print(trans_dataset)

from sklearn.model_selection import train_test_split

X = dataset2.drop("yield", axis=1)
y = target

X_train, X_test, y_train, y_test = train_test_split(X, y,
                      test_size=0.2, random_state=42)
X_test

#LinearRegression

from sklearn.linear_model import LinearRegression
from sklearn import metrics
from sklearn.metrics import r2_score

lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
pred_lr = lr_model.predict(X_test)
lr_score = r2_score(y_test, pred_lr)
lr_score

plt.figure(figsize=(10,5))
sns.set(rc={'axes.facecolor':'White','axes.grid':False,'xtick.labelsize':9,'ytick.labelsize':9})
sns.regplot(x=y_test,y=pred_lr,data=dataset2, scatter_kws={"color": "#FA1D19"}, line_kws={"color": "black"})
plt.title("PREDICTION Linear Regression")
plt.show()

print('lr_prediction',pred_lr)
print('y_test',y_test)

# RandomForestRegressor
from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train, y_train)
pred_rf = rf_model.predict(X_test)
rf_score = r2_score(y_test, pred_rf)
rf_score

plt.figure(figsize=(10,5))
sns.set(rc={'axes.facecolor':'White','axes.grid':False,'xtick.labelsize':9,'ytick.labelsize':9})
sns.regplot(x=y_test,y=pred_rf,data=dataset2, scatter_kws={"color": "#FA1D19"}, line_kws={"color": "black"})
plt.title("PREDICTION RandomForestRegressor")
plt.show()

# DecisionTreeRegressor
from sklearn.tree import DecisionTreeRegressor

dtree_reg = DecisionTreeRegressor()
dtree_reg.fit(X_train, y_train)
pred_dtree = dtree_reg.predict(X_test)
dtree_score = r2_score(y_test, pred_dtree)
dtree_score

plt.figure(figsize=(10,5))
sns.set(rc={'axes.facecolor':'White','axes.grid':False,'xtick.labelsize':9,'ytick.labelsize':9})
sns.regplot(x=y_test,y=pred_dtree,data=dataset2, scatter_kws={"color": "#FA1D19"}, line_kws={"color": "black"})
plt.title("PREDICTION DecisionTreeRegressor")
plt.show()

"""以下不將fruit 、seed套入模型"""

dataset3 = dataset2.drop(['fruitset','fruitmass',
              'seeds'],1)
dataset3

from sklearn.model_selection import train_test_split

X = dataset3.drop("yield", axis=1)
y = target

X_train, X_test, y_train, y_test = train_test_split(X, y,
                      test_size=0.2, random_state=42)
X_test

lr_model2 = LinearRegression()
lr_model2.fit(X_train, y_train)
pred_lr2 = lr_model2.predict(X_train)
lr_score2 = r2_score(y_test, pred_lr)
lr_score2

dataset4 = dataset2.drop(['honeybee','bumbles',
              'andrena','osmia'],1)
dataset4

from sklearn.model_selection import train_test_split

X = dataset4.drop("yield", axis=1)
y = target

X_train, X_test, y_train, y_test = train_test_split(X, y,
                      test_size=0.2, random_state=42)
X_test

# Get feature importance
importance = train_results['LightGBM_pipelines'][0].model.feature_importances_
for i in range(1, len(train_results['LightGBM_pipelines'])):
    importance += train_results['LightGBM_pipelines'][i].model.feature_importances_
importance = importance / 5.0
feature_names = X.columns.values

# Create a dataframe of feature importance
df_importance = pd.DataFrame({'feature': feature_names, 'importance': importance})
# Sort features by importance
df_importance = df_importance.sort_values('importance', ascending=False)

# Create horizontal bar plot of feature importance
sns.set_style("whitegrid")
plt.figure(figsize=(10, 8))
palette = sns.color_palette("husl", len(df_importance))
sns.barplot(x="importance", y="feature", data=df_importance, palette=palette)
plt.title('Feature Importances')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

from sklearn.ensemble import ExtraTreesClassifier
feature_cols = ['BGI_1km', 'BGI_2km', 'BGI_5km', 'fodder_monthsperyear', 'ntfp_monthsperyear', 'treeperhh_1km', 'treeperhh_2km', 'treeperhh_5km', 'pcnttree_1km', 'pcnttree_2km', 'pcnttree_5km', 'dist_city', 'dist_road']

# df['sum_score'] = df[feature_cols].sum(1)

# print(df['sum_score'].describe())
# df.loc[df['sum_score'] < 4.76,'sum_score'] = 0
# df.loc[df['sum_score'] >= 4.76,'sum_score'] = 1
# print(df['sum_score'].describe())

# define X and y

X = df[feature_cols]
y = target



# split X and y into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=40)

# Create dictionaries for final graph
# Use: methodDict['Stacking'] = accuracy_score
methodDict = {}
rmseDict = ()

# Build a forest and compute the feature importances
forest = ExtraTreesClassifier(n_estimators=250,
                              random_state=0)

forest.fit(X, y)
importances = forest.feature_importances_
std = np.std([tree.feature_importances_ for tree in forest.estimators_],
             axis=0)
indices = np.argsort(importances)[::-1]

labels = []
for f in range(X.shape[1]):
    labels.append(feature_cols[f])

# Plot the feature importances of the forest
plt.figure(figsize=(12,8))
plt.title("Feature importances")
plt.bar(range(X.shape[1]), importances[indices],
       color="r", yerr=std[indices], align="center")
plt.xticks(range(X.shape[1]), labels, rotation='vertical')
plt.xlim([-1, X.shape[1]])
plt.show()